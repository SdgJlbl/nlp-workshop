{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words is not the only way to encode text. Word embeddings are a mapping from text to a high-dimensional vector space. Contrary to bag of words, word embeddings are a dense representation, and it retains some sense of similarity / closeness between words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A bit of theory\n",
    "\n",
    "The idea behind word embeddings is, as J. R. Firth said, \"to know a word by the company it keeps\". \n",
    "\n",
    "Embeddings are learned using neural networks. There are two ways of training the networks:\n",
    " 1. Learn to predict the target word given the context (continuous bag of words, CBOW).\n",
    " 2. Learn to predict words in the context windows given the target words (skipgram).\n",
    "\n",
    "CBOW is usually faster to train, but skipgram gives better results, especially on infrequent words. \n",
    "\n",
    "Word Embeddings requires a very large corpus of texts to be learned, that is why most of the time we use pretrained embeddings, such as Word2Vec or GloVe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play around with word embeddings\n",
    "\n",
    "Now, let's load an embedding and run a few tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to download Word2Vec embedding [here](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gzip -d data/GoogleNews-vectors-negative300.bin.gz -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "wordVectors = KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check how similar some words are for this embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordVectors.similarity('apple', 'banana')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordVectors.similarity('apple', 'python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordVectors.similarity('python', 'snake')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One nice feature of word embeddings is that in addition of similarities, they retain some analogies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordVectors.similar_by_vector(wordVectors['king']- wordVectors['man'] + wordVectors['woman'], topn=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Word Embeddings to our dataset\n",
    "\n",
    "We have a new way of encoding text, how can we apply it to our initial classification task? \n",
    "\n",
    "If we load our dataset again, each sample is a list of words, to which correspond a numerical vector. Numerical vectors is something we can work with, except that there is a different number of words in each sample, which is not something we can easily keep in a matrix. \n",
    "\n",
    "The solution that is often used is to create a document vector, computed by averaging word vectors in the extract. Let's implement that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                      remove=('headers', 'footers', 'quotes'))\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',\n",
    "                                     remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to do a bit of preprocessing that was previously handled for us by scikit-learn. We will tokenize the text (split the text into words), convert all strings to lower-case, get rid of non-alphabetical strings and of stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_en = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(string):\n",
    "    tokens = wordpunct_tokenize(string)\n",
    "    return [token.lower() for token in tokens if (token.isalpha() and token.lower() not in stopwords_en)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what our cleaning function does on the first extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text(newsgroups_train.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can write the function to compute the document vector for each text extract, and use it to build the encoded training set. \n",
    "\n",
    "The document vector is computed by taking the average of all word vectors present in the extract.\n",
    "\n",
    "Don't forget to also encode the test set! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/word_encoding.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify with Logistic Regression\n",
    "\n",
    "Now we can try and apply Logistic Regression as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/classify_word_embedding.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/confusion_matrix.py\n",
    "from sklearn.metrics import confusion_matrix as sk_confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "def confusion_matrix(y_true, y_predicted, labels):\n",
    "    df = pd.DataFrame(data=sk_confusion_matrix(y_true, y_predicted), index=labels, columns=labels)\n",
    "    df.index.name = 'true classes'\n",
    "    df.columns.name = 'predicted classes'\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_estimated_test = lr_classifier.predict(X_test) \n",
    "confusion_mat = confusion_matrix(newsgroups_test.target, y_estimated_test, newsgroups_test.target_names)\n",
    "confusion_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.heatmap(confusion_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have improved our accuracy score with respect to the bag-of-words approach. \n",
    "\n",
    "Our confusion matrix still looks similar, though, we are still having a hard time to classify `talk.religion.misc` extracts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
