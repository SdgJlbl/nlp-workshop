{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words approach for Natural Language Processing\n",
    "\n",
    "We have seen in the last workshop how to train a model on tabular data. We saw that it was straightforward on numerical columns, but that it required a bit more work on categorical data. \n",
    "\n",
    "Well, what happens if we only have non-numerical data, like text, or images? This kind of data is called unstructured data, because it does not fit nicely in a table. \n",
    "\n",
    "In this tutorial, we will try to classify some text extracts into some given categories - so it's supervised learning like last time, only with text. \n",
    "\n",
    "Natural Language Processing typically requires a lot of preprocessing from the raw dataset, which we are today going to gloss over, in the interest of time. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a classic dataset for text classification, 20 newsgroup. It contains extracts from 20 Usenet newsgroup, and the goal is to predict to which newsgroup a certain extract belongs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a look at the 20 categories of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are directly extracted from the Usenet newsgroup hierarchy, so we can still see the tree structure.\n",
    "\n",
    "There is a wide variety of categories, some of them closely related, like `comp.sys.ibm.pc.hardware` and `comp.sys.mac.hardware`, some of them are quite unique, like `misc.forsale`, and some of them are opposing pairs, like `alt.atheism` and `soc.religion.christian`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the kind of text we want to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm, that's kind of messy, there is a lot of \"metadata\", or at least information we don't really want our classifier to learn about (such as email addresses, or newsgroup header). \n",
    "\n",
    "Fortunately, scikit-learn has implemented the cleaning step for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                      remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better. Now the text extract is clean and contains only the content of the message. \n",
    "\n",
    "But how do we handle it from there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first way of encoding text is to use an approach called bag of words:\n",
    "* We define a vocabulary.\n",
    "* For each text extract to classify, we count the number of occurences of each word, and fill the appropriate index in the vocabulary vector.\n",
    "* All words which are in the vocabulary but not in the text get a 0 value. Words that are in the text but not in the vocabulary are ignored.\n",
    "* The (sparse) matrix made of concatenating those vectors (size n_samples x vocabulary_size) is then fed to the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example. We have the following dataset:\n",
    "\n",
    "| index | text                         |\n",
    "|-------|------------------------------|\n",
    "| **1** | All cats are mortal.         |\n",
    "| **2** | Socrates is mortal.          |\n",
    "| **3** | Therefore Socrates is a cat. |\n",
    "\n",
    "We define our vocabulary to be:\n",
    "```\n",
    "    voc = ['Socrates', 'cat', 'cats', 'mortal', 'therefore']\n",
    "```\n",
    "\n",
    "Then, our encoded matrix is:\n",
    "\n",
    "| index | Socrates | cat | cats | mortal | therefore |\n",
    "|-------|----------|-----|------|--------|-----------|\n",
    "| **1** | 0        | 0   | 1    | 1      | 0         |\n",
    "| **2** | 1        | 0   | 0    | 1      | 0         |\n",
    "| **3** | 1        | 1   | 0    | 0      | 1         |\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm sure that all the Pythonistas already have an idea of how to implement this bag of words encoding with `Counter` and a clever list comprehension. \n",
    "\n",
    "We are not going to do that here (but feel welcome to give it a try at home), scikit-learn will take care of that for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a look at the size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, more than 100 000 words, that's a lot ! \n",
    "\n",
    "Without further instructions, the `CountVectorizer` keeps in the vocabulary every word appearing even only once in the whole corpus. That's not quite what we want.\n",
    "\n",
    "As a rule of thumb, we prefer to train on \"long\" matrices (with a lot of rows), rather than on \"wide\" matrices (with a lot of columns). The intuition behind that is that, if we have enough columns, the classifier will probably learn by heart a unique combination for each row, and thus not generalize well. That is overfitting! \n",
    "\n",
    "So, to force the classifier to generalize, we want to feed it a matrix with less columns, i.e. less words in the vocabulary. How about we keep the 3000 most frequent words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=3000)\n",
    "vectorizer.fit(newsgroups_train.data)\n",
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the words in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(vectorizer.vocabulary_)[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, people sure are \"wondering if anyone out there could \\[help\\] them\". \n",
    "\n",
    "Joke aside, we can see that some words in the vocabulary, such as `was`, `this`, `the`, which do not carry a lot of semantic meaning. We call those stopwords, and we don't want to keep them in our limited vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=3000,\n",
    "                             stop_words='english')\n",
    "vectorizer.fit(newsgroups_train.data)\n",
    "list(vectorizer.vocabulary_)[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks better. \n",
    "Now we can encode our dataset with this vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.transform(newsgroups_train.data)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the sparse matrix we get looks correct, can we check the first row?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, col_index = X_train[0].nonzero()\n",
    "for i in col_index:\n",
    "    print(sorted(list(vectorizer.vocabulary_))[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems good. It's time to train our classifier !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first classifier we will use is called Naive Bayes. It uses Bayes rule to make decisions:\n",
    "$$P(y=C_i | (\\mbox{features}) =  \\frac{P((\\mbox{features } | y=C_i) P(y=C_i)}{P((\\mbox{features})}$$\n",
    "\n",
    "It supposes that all features (here, the presence or the absence of a word in the text) are conditionally independent (that's the naive part). \n",
    "\n",
    "So the decision function can be rewritten as:\n",
    "$$ P(y=C_i | (\\mbox{features}) \\propto \\prod_\\mbox{feature} P(\\mbox{feature } | y=C_i) P(y=C_i) $$\n",
    "since $P(\\mbox{features})$ is a constant that we do not need to compute.\n",
    "\n",
    "$P(y=C_i) $ is just the relative frequency of the class $C_i$ in the training set, we only need to compute the $P(\\mbox{feature } | y=C_i)$ for each feature.\n",
    "\n",
    "In the multinomial flavour we are using here, the likelihood to each feature given a class is simply computed using a smoothed relative frequency count. (See [scikit-learn documentation](http://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes) for more details.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, newsgroups_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train error\n",
    "nb_classifier.score(X_train, newsgroups_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our classifier is trained, we want to evaluate how good it would be on new data. We can get a test set using the option `subset='test'`, now it's your time to do the preprocessing again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/preprocessing_test.py\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',\n",
    "                                      remove=('headers', 'footers', 'quotes'))\n",
    "X_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now compute the test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/test_score_nb.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an overall accuracy of 70% on the training set, and only 55% on the test set. It might seem quite low, but let's not forget that we have 20 classes, so if we were making random predictions, we would have an accuracy of roughly 5%.\n",
    "\n",
    "We can check that by computing a random baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy = DummyClassifier()\n",
    "dummy.fit(X_train, newsgroups_train.target)\n",
    "dummy.score(X_train, newsgroups_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy.score(X_test, newsgroups_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing we might want to check is how errors are distributed along classes. For example, it is understandable that the classifier would confuse extract from `comp.sys.ibm.pc.hardware` and `comp.sys.mac.hardware`. \n",
    "\n",
    "The tool to visualize that is called a confusion matrix. We define a function to prettify scikit-learn output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/confusion_matrix.py\n",
    "from sklearn.metrics import confusion_matrix as sk_confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "def confusion_matrix(y_true, y_predicted, labels):\n",
    "    df = pd.DataFrame(data=sk_confusion_matrix(y_true, y_predicted), index=labels, columns=labels)\n",
    "    df.index.name = 'true classes'\n",
    "    df.columns.name = 'predicted classes'\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_estimated_test = nb_classifier.predict(X_test) \n",
    "confusion_mat = confusion_matrix(newsgroups_test.target, y_estimated_test, newsgroups_test.target_names)\n",
    "confusion_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the matrix as a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.heatmap(confusion_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_estimated_test = nb_classifier.predict(X_test) \n",
    "confusion_mat = confusion_matrix(newsgroups_test.target, y_estimated_test, newsgroups_test.target_names)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use other classifiers than Naive Bayes. A personal favorite of mine is Logistic Regression, which is the most badly named linear classifier ever, but it has the advantage of retaining some interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_classifier = LogisticRegression(multi_class='multinomial',\n",
    "                                   solver='lbfgs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can train and evaluate this classifier. Remember that all scikit-learn classifier share a common interface, so you can probably use the same method as for the Naive Bayes classifier (or you can check the documentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/log_reg_training.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have improved a lot our train accuracy! Unfortunately, that does not transfer to the test accuracy, we are probably overfitting. \n",
    "\n",
    "The main hyperparameter of logistic regression is called `C`, it's a positive float which is the inverse of the regularisation strength. The smaller `C`, the smoother our decision function will be, which means we are less likely to overfit.\n",
    "\n",
    "Let's modify our code to add some regularisation. `C` defaults to `1`, we want more regularisation, let's try something smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/log_reg_training_with_regularisation.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the exact value you chose, the test accuracy might be slightly better or worse, but how can we choose the optimal value for `C`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use cross-validation. If you don't remember what is cross-validation, here is a quick summary.\n",
    "\n",
    "    At each iteration, we use 90% of the data to train a model, and the remaining 10% to evaluate how good the model is. \n",
    "    And we repeat that 10 times, using a different 10% to evaluate each time. \n",
    "\n",
    "![Cross-validation](img/crossValidation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the good news is that we don't even have to do that by hand, scikit-learn provides us with a class for that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/log_reg_cv_training.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mat = confusion_matrix(newsgroups_test.target, lr_cv_classifier.predict(X_test), newsgroups_test.target_names)\n",
    "confusion_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if the improvement did not seem like much on the accuracy value, the confusion matrix looks much nicer with the Logistic Regression. \n",
    "\n",
    "Some samples are predicted for the `comp.os.ms-windows.misc` class and the classifier is much less confused with the `comp` classes which were mixed before. \n",
    "\n",
    "The improvement is not that spectacular with the class `talk.religion.misc`, but it is not worse. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus step: Optimise the values of hyperparameters with cross-validation**\n",
    "\n",
    "`C` is not the only hyperparameter we can tune with cross-validation. Remember that we chose the number of words in the vocabulary at the beginning of the notebook, and that choice was quite arbitrary too. \n",
    "\n",
    "We can tune both these parameters to improve accuracy, and the best way would be to tune them at the same time. \n",
    "\n",
    "Here are some hints to solve this advanced exercise:\n",
    " * We want to do a grid search over those two hyperparameters, that is to say, try every possible combination and keep the best one.\n",
    " * scikit-learn can probably help you, have a look at the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n",
    " * Consider using `sklearn.pipeline.Pipeline` to merge both steps (vectorizer and classifier) into one estimator.\n",
    " * The number of models to train grows as the cartesian product of the two lists of hyperparameters to try, don't be too greedy! Trying out 3 values for the vocabulary size (1500, 3000, 5000) and 5 values for `C` (.001, .01, .05, .1, .5) is probably a good start. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/grid_search.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One great thing about using intepretable classifiers such as logisitic regression is that we can have a look at why the classifier is making the prediction it makes. \n",
    "\n",
    "We are going to use a Python library called eli5 which provides tools to visualize the inner workings of ML algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "eli5.show_weights(lr_cv_classifier, \n",
    "                  vec=vectorizer, \n",
    "                  top=10,\n",
    "                  target_names=newsgroups_test.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that often the features on which the logistic regression relies make sense: we find `bike` in the `rec.motorcycles` class, `encryption` for the `sci.crypt` class, and so on. \n",
    "\n",
    "We can also observe that `soc.religion.christian` and `talk.religion.misc` share quite a lot of features, which explains why the classifier is so confused between the two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eli5 has another cool feature which is explaining the prediction for a given sample. Words highlighted in red are contributing negatively to the class (making the class less likely), and words in green are contributing towards the class. The deeper the color, the higher the contribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_prediction(lr_cv_classifier, \n",
    "                     newsgroups_test.data[0], \n",
    "                     vec=vectorizer,\n",
    "                     target_names=newsgroups_test.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a lot of classes, so that's a bit messy. We can only show the top 5 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_prediction(lr_cv_classifier, \n",
    "                     newsgroups_test.data[0], \n",
    "                     vec=vectorizer,\n",
    "                     target_names=newsgroups_test.target_names, \n",
    "                     top_targets = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also have a look at a sample that is wrongly classified to get a sense of what went wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_test.target_names[newsgroups_test.target[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_prediction(lr_cv_classifier, \n",
    "                     newsgroups_test.data[1], \n",
    "                     vec=vectorizer,\n",
    "                     target_names=newsgroups_test.target_names, \n",
    "                     top_targets = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One reason for misclassification might be that words are cut out in two when they contains a dash. That's something we might be able to fix by changing options in `CountVectorizer`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen how to encode text using bag of words. One advantage of this approach is that, if combined with the right classifier, the results remain interpretable.\n",
    "\n",
    "We have seen that many machine learning algorithms, whether we are dealing with text or not, have a lot of hyperparameters to fine tune. But fortunately, there are existing tools to help us with this endeavour.\n",
    "\n",
    "What we have not seen: all the Natural Language Processing necessary for handling raw text: tokenization (splitting the text into words), lemmatization (stripping words to their base form, 'was' -> 'be'), cleaning of irrelevant words, ect..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
