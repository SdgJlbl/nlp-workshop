{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "Classifying extracts from newsgroups is all fine and well, but it's not really mind-blowing. In addition, it requires to have a lot of data already labelled, which is not typically the case in real life applications. \n",
    "\n",
    "For the last part of this tutorial, we will try to do something fancier. We will use Word Embeddings to learn a very simple Sentiment Analysis classifier. The goal is to be able to score how positive or negative an extract is on a one-dimensional scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do that, we will train a classifier on a list of more than 6000 words, split between 'positive' and 'negative' words. Of course, sentiment associated with a word is very context-dependent, but we will see what we can do in this simplified case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_lexicon = pd.read_csv('data/sentiment_lexicon.csv', index_col=0)\n",
    "sentiment_lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels are 1 for positive words, and 0 for negative words.  Columns 0 to 299 contain the word vectors associated to each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a sentiment analysis classifier\n",
    "\n",
    "First, we will split our sentiment dataset between a training set and a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(sentiment_lexicon.drop(['label'], axis=1).values,\n",
    "                                                  sentiment_lexicon['label'],\n",
    "                                                  test_size=.25,\n",
    "                                                  stratify=sentiment_lexicon['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we train our classifier on the training set. Since we have only two classes, it's a binary classifier which is much easier to train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "lr_classifier = LogisticRegressionCV()\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "print('Optimal C value', lr_classifier.C_[0])\n",
    "print('train accuracy', \n",
    "     lr_classifier.score(X_train, y_train),\n",
    "     '\\nvalidation accuracy',\n",
    "     lr_classifier.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a pretty high accuracy, let's have a look on how the classifier generalizes on unseen words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_to_sentiment(vec):\n",
    "    # predict_log_proba gives the log probability for each class\n",
    "    predictions = lr_classifier.predict_log_proba(vec)\n",
    "\n",
    "    # To see an overall positive vs. negative classification in one number,\n",
    "    # we take the log probability of positive sentiment minus the log\n",
    "    # probability of negative sentiment.\n",
    "    return predictions[:, 1] - predictions[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_words = y_val.to_frame()\n",
    "validation_words['sentiment'] = vec_to_sentiment(X_val)[:, None]\n",
    "validation_words.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to generalize all right. \n",
    "\n",
    "Now we want to apply this classifier on whole sentences, by taking the average sentiment value for all word embeddings in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "wordVectors = KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "stopwords_en = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(string):\n",
    "    tokens = wordpunct_tokenize(string)\n",
    "    return [token.lower() for token in tokens if (token.isalpha() and token.lower() not in stopwords_en)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/words_to_sentiment.py\n",
    "\n",
    "def words_to_sentiment(sentence):\n",
    "    # Given a string representing a sentence, we want to split it into clean tokens with the clean_text function\n",
    "    # Then we apply the vec_to_sentiment function to each token (if present in the vocabulary) to get a sentiment value\n",
    "    # We return the average sentiment value\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if it is working on whole sentences now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_sentiment('I am happy and joyous!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_sentiment('I feel okay.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_sentiment('This is a sad day...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that's great! (Sentiment score 4.78) \n",
    "\n",
    "Can we try on more sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_sentiment(\"I want to visit France\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_sentiment(\"I want to visit Japan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_sentiment(\"I want to visit Congo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_sentiment(\"I want to visit Iraq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_sentiment(\"I like Italian food\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_sentiment(\"I like Mexican food\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These sets of sentences should have similar sentiment scores, because they express the same objective idea. \n",
    "\n",
    "But actually, since word embeddings are trained on real-life corpuses containing prejudices and bias, they also learn these biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inspiration for this third part is a tutorial that delves more deeply into this subject, available [here](http://blog.conceptnet.io/posts/2017/how-to-make-a-racist-ai-without-really-trying/). \n",
    "\n",
    "Some papers on the subject:\n",
    " - [*Semantics derived automatically from language corpora contain human-like biases*](https://researchportal.bath.ac.uk/en/publications/semantics-derived-automatically-from-language-corpora-necessarily),  Aylin Caliskan, Joanna J Bryson, Arvind Narayanan\n",
    " - [*Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings*](https://arxiv.org/abs/1607.06520), Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, Adam Kalai\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Word Embeddings are a powerful way to encode text, even though they requires quite a lot of memory to load.\n",
    "\n",
    "But with great power comes great responsability. Word Embeddings are often biaised, and you need to consider the influence of these biases on your application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
